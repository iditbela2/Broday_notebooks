{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import ipywidgets as widgets\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_absolute_error #=mean error (Simon 2012)\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import rcParams, cycler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(threshold, start_time):\n",
    "    PM25 = pd.read_pickle(\"/Users/iditbela/Documents/Broday/saved_data_from_notebooks/PM25\")\n",
    "\n",
    "    times = pd.date_range(start=start_time, end='2018-12-31 23:00:00', freq='30Min') #one less because the last is always nan\n",
    "\n",
    "    start_idx = PM25.shape[0]-times.shape[0]\n",
    "\n",
    "    # remove the last index as it is always nan\n",
    "    PM25 = PM25[:-1]\n",
    "    times = times[:-1]\n",
    "\n",
    "    # reduced PM25 \n",
    "    r_PM25 = PM25[start_idx:] \n",
    "    idx = r_PM25.notnull().sum(axis = 0)/r_PM25.shape[0]>threshold\n",
    "    r_PM25 = r_PM25.loc[:, idx]\n",
    "\n",
    "    r_PM25.reset_index(inplace=True)\n",
    "    r_PM25.drop(labels = 'index',axis=1, inplace=True)\n",
    "    \n",
    "    return times, r_PM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a np_array np_y_missing, fill the np_y_missing with values where the validation\n",
    "# indexes are chosen. \n",
    "\n",
    "def get_validation_index(PM25, interval_length, s, exclude):\n",
    "    \n",
    "    idx_start = np.random.choice([i for i in range(0,len(PM25.iloc[:,s])) if i not in exclude])\n",
    "    condition = PM25.iloc[idx_start:idx_start+interval_length,s].isnull().rolling(interval_length,min_periods=0).sum().max()\n",
    "\n",
    "    # I allow some nan values (0.5%) to be inside in validation interval. \n",
    "    while ((condition>interval_length/100/2) or (np.isnan(PM25.iloc[idx_start-1,s])) or (np.isnan(PM25.iloc[idx_start+interval_length+1,s])) or ((idx_start+interval_length+1)>len(PM25.iloc[:,s]))):\n",
    "        idx_start = np.random.choice([i for i in range(0,len(PM25.iloc[:,s])) if i not in exclude])\n",
    "        condition = PM25.iloc[idx_start:idx_start+interval_length,s].isnull().rolling(interval_length,min_periods=0).sum().max()\n",
    "\n",
    "    return (idx_start,idx_start + interval_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that returns X_missing, y_missing (splits the data)\n",
    "\n",
    "\n",
    "def return_X_y_for_validation(PM25,IL):\n",
    "    \n",
    "    np_r_PM25_y_mask = PM25.copy().values\n",
    "    np_r_PM25_y_mask[:] = 0\n",
    "    \n",
    "    for s in range(np_r_PM25_y_mask.shape[1]-33):\n",
    "        \n",
    "        total_exclude = [] # a list of excluded indexes (to not overlap the chunks. not \n",
    "        # sure it is important if I evaluate seperately for each chunk size).\n",
    "        \n",
    "        # 720 hours(30 days)(1)\n",
    "        interval_length = IL[0]*2\n",
    "        (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "        total_exclude.append(range(a,b))\n",
    "        np_r_PM25_y_mask[a:b,s] = 1\n",
    "\n",
    "        # 120 hours (5 days)(6)\n",
    "        interval_length = IL[1]*2\n",
    "        for i in tqdm(range(6)):\n",
    "            (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "            total_exclude.append(range(a,b))\n",
    "            np_r_PM25_y_mask[a:b,s] = 2\n",
    "\n",
    "        # 24 hours (1 day)(30)\n",
    "        interval_length = IL[2]*2\n",
    "        for i in tqdm(range(30)):\n",
    "            (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "            total_exclude.append(range(a,b))\n",
    "            np_r_PM25_y_mask[a:b,s] = 3\n",
    "\n",
    "        # 12 hours(60)\n",
    "        interval_length = IL[3]*2\n",
    "        for i in tqdm(range(60)):\n",
    "            (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "            total_exclude.append(range(a,b))\n",
    "            np_r_PM25_y_mask[a:b,s] = 4\n",
    "\n",
    "        # 2 hours(360) \n",
    "        interval_length = IL[4]*2\n",
    "        for i in tqdm(range(360)):\n",
    "            (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "            total_exclude.append(range(a,b))\n",
    "            np_r_PM25_y_mask[a:b,s] = 5\n",
    "\n",
    "        # 1 hour(720)\n",
    "        interval_length = IL[5]*2\n",
    "        for i in tqdm(range(720)):\n",
    "            (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "            total_exclude.append(range(a,b))\n",
    "            np_r_PM25_y_mask[a:b,s] = 6\n",
    "\n",
    "        # 0.5 hour(1440)\n",
    "        interval_length = int(IL[6]*2)\n",
    "        for i in tqdm(range(1440)):\n",
    "            (a,b) = get_validation_index(PM25, interval_length, s, total_exclude)\n",
    "            total_exclude.append(range(a,b))\n",
    "            np_r_PM25_y_mask[a:b,s] = 7\n",
    "\n",
    "        \n",
    "    return np_r_PM25_y_mask  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a function that imputes (and saves imputation results)\n",
    "\n",
    "# a function that cumputes validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6 # how much non-missing values are in the time-series in order to include the station?\n",
    "start_time = '2013-01-01 00:00:00'\n",
    "times, r_PM25 = initialize(threshold, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IL = [720,120,24,12,2,1,0.5]   \n",
    "# IL = [1200, 100, 50, 10, 5, 1, 0.5]\n",
    "\n",
    "np_r_PM25_y_mask = return_X_y_for_validation(r_PM25,IL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m (number of iterations) should be approximately as the average missing percentage. \n",
    "# which is about 15% in my case. ok to start with 5-10 though. \n",
    "# loose the 5 rows where all rows are empty!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ways to check randomness - https://www.theanalysisfactor.com/missing-data-mechanism/\n",
    "# mark missing values in 1 and non-missing in zero. run a t-test or chi-square test \n",
    "# between one variable(column) and all other variables(columns). I think it should\n",
    "# tell me if they are drown from the same poplutation (I want p-value not significant,\n",
    "# meaning they are from different populations). if they come from the same population \n",
    "# I suspect that it is not random.\n",
    "# MAYBE THE 0-1 IS TO TEST THE ORDER OF THE VALUES? IF IN RANDOM. = TEST FOR MCAR\n",
    "# another option (I think test for MAR) is if I suspect that it is not completely random \n",
    "# (due to malfunctioning in a certain\n",
    "# time maybe), I can try to train a logistic regression to see if time of day can \n",
    "# predict the missing non-missing classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of the imputed values Vs. the dist. of the non-missing values.\n",
    "\n",
    "\n",
    "'''The idea is that good imputations have a distribution \n",
    "similar to the observed data. In other words, the imputations\n",
    "could have been real values had they been observed. \n",
    "Except under MCAR, the distributions do not need to be identical, \n",
    "since strong MAR mechanisms may induce systematic differences between \n",
    "the two distributions. However, any dramatic differences between the \n",
    "imputed and observed data should certainly alert us to the possibility\n",
    "that something is wrong.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stefvanbuuren.name/fimd/sec-nutshell.html\n",
    "# http://www.stat.columbia.edu/~gelman/arm/missing.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
